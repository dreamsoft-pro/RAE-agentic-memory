apps/memory_api/repositories/evaluation_repository.py:257: error: Returning Any from function declared to return "bool"  [no-any-return]
apps/memory_api/repositories/evaluation_repository.py:572: error: Returning Any from function declared to return "bool"  [no-any-return]
rae-core/rae_core/math/structure.py:181: error: Dict entry 4 has incompatible type "str": "str"; expected "str": "float"  [dict-item]
rae-core/rae_core/reflection/reflector.py:95: error: Need type annotation for "tag_counts" (hint: "tag_counts: dict[<type>, <type>] = ...")  [var-annotated]
rae-core/rae_core/reflection/reflector.py:154: error: Need type annotation for "tag_freq" (hint: "tag_freq: dict[<type>, <type>] = ...")  [var-annotated]
rae-core/rae_core/reflection/reflector.py:205: error: Need type annotation for "layer_dist" (hint: "layer_dist: dict[<type>, <type>] = ...")  [var-annotated]
rae-core/rae_core/reflection/reflector.py:278: error: Need type annotation for "tag_groups" (hint: "tag_groups: dict[<type>, <type>] = ...")  [var-annotated]
rae-core/rae_core/reflection/evaluator.py:92: error: Returning Any from function declared to return "dict[str, float]"  [no-any-return]
rae-core/rae_core/adapters/postgres.py:276: error: Signature of "list_memories" incompatible with supertype "rae_core.interfaces.storage.IMemoryStorage"  [override]
rae-core/rae_core/adapters/postgres.py:276: note:      Superclass:
rae-core/rae_core/adapters/postgres.py:276: note:          def list_memories(self, tenant_id: str, agent_id: str | None = ..., layer: str | None = ..., tags: list[str] | None = ..., filters: dict[str, Any] | None = ..., limit: int = ..., offset: int = ..., order_by: str = ..., order_direction: str = ...) -> Coroutine[Any, Any, list[dict[str, Any]]]
rae-core/rae_core/adapters/postgres.py:276: note:      Subclass:
rae-core/rae_core/adapters/postgres.py:276: note:          def list_memories(self, tenant_id: str, agent_id: str | None = ..., layer: str | None = ..., filters: dict[str, Any] | None = ..., limit: int = ..., offset: int = ..., order_by: str = ..., order_direction: str = ...) -> Coroutine[Any, Any, list[dict[str, Any]]]
rae-core/rae_core/adapters/postgres.py:374: error: Returning Any from function declared to return "bool"  [no-any-return]
rae-core/rae_core/adapters/postgres.py:397: error: Returning Any from function declared to return "bool"  [no-any-return]
rae-core/rae_core/adapters/postgres.py:446: error: Returning Any from function declared to return "bool"  [no-any-return]
rae-core/rae_core/adapters/postgres.py:478: error: Returning Any from function declared to return "bool"  [no-any-return]
rae-core/rae_core/adapters/postgres.py:498: error: Returning Any from function declared to return "bool"  [no-any-return]
rae-core/rae_core/adapters/postgres.py:619: error: Signature of "count_memories" incompatible with supertype "rae_core.interfaces.storage.IMemoryStorage"  [override]
rae-core/rae_core/adapters/postgres.py:619: note:      Superclass:
rae-core/rae_core/adapters/postgres.py:619: note:          def count_memories(self, tenant_id: str, agent_id: str | None = ..., layer: str | None = ...) -> Coroutine[Any, Any, int]
rae-core/rae_core/adapters/postgres.py:619: note:      Subclass:
rae-core/rae_core/adapters/postgres.py:619: note:          def count_memories(self, tenant_id: str, agent_id: str, layer: str) -> Coroutine[Any, Any, int]
rae-core/rae_core/reflection/engine.py:98: error: Unsupported operand types for + ("object" and "int")  [operator]
rae-core/rae_core/reflection/engine.py:114: error: Unsupported operand types for + ("object" and "int")  [operator]
rae-core/rae_core/reflection/engine.py:123: error: Unsupported operand types for + ("object" and "int")  [operator]
apps/memory_api/models/dashboard_models.py:496: error: Incompatible types in assignment (expression has type "str", base class "WebSocketMessage" defined the type as "DashboardEventType")  [assignment]
rae-core/rae_core/sync/merge.py:307: error: Unsupported operand types for + ("object" and "int")  [operator]
rae-core/rae_core/sync/merge.py:309: error: Unsupported operand types for + ("object" and "int")  [operator]
rae-core/rae_core/sync/merge.py:310: error: "object" has no attribute "append"  [attr-defined]
rae-core/rae_core/sync/merge.py:314: error: Unsupported operand types for + ("object" and "int")  [operator]
rae-core/rae_core/sync/merge.py:316: error: "object" has no attribute "append"  [attr-defined]
apps/memory_api/services/compliance_service.py:937: error: Returning Any from function declared to return "float"  [no-any-return]
apps/memory_api/services/compliance_service.py:1012: error: Returning Any from function declared to return "float"  [no-any-return]
apps/memory_api/services/compliance_service.py:1149: error: Returning Any from function declared to return "float"  [no-any-return]
rae-core/rae_core/adapters/memory/storage.py:47: error: Signature of "store_memory" incompatible with supertype "rae_core.interfaces.storage.IMemoryStorage"  [override]
rae-core/rae_core/adapters/memory/storage.py:47: note:      Superclass:
rae-core/rae_core/adapters/memory/storage.py:47: note:          def store_memory(self, content: str, layer: str, tenant_id: str, agent_id: str, tags: list[str] | None = ..., metadata: dict[str, Any] | None = ..., embedding: list[float] | None = ..., importance: float | None = ..., expires_at: Any | None = ...) -> Coroutine[Any, Any, UUID]
rae-core/rae_core/adapters/memory/storage.py:47: note:      Subclass:
rae-core/rae_core/adapters/memory/storage.py:47: note:          def store_memory(self, content: str, layer: str, tenant_id: str, agent_id: str, tags: list[str] | None = ..., metadata: dict[str, Any] | None = ..., embedding: list[float] | None = ..., importance: float | None = ...) -> Coroutine[Any, Any, UUID]
rae-core/rae_core/adapters/memory/storage.py:139: error: Invalid index type "tuple[str, Any | None]" for "dict[tuple[str, str], set[UUID]]"; expected type "tuple[str, str]"  [index]
rae-core/rae_core/adapters/memory/storage.py:140: error: Invalid index type "tuple[str, Any | None]" for "dict[tuple[str, str], set[UUID]]"; expected type "tuple[str, str]"  [index]
rae-core/rae_core/adapters/memory/storage.py:174: error: Signature of "list_memories" incompatible with supertype "rae_core.interfaces.storage.IMemoryStorage"  [override]
rae-core/rae_core/adapters/memory/storage.py:174: note:      Superclass:
rae-core/rae_core/adapters/memory/storage.py:174: note:          def list_memories(self, tenant_id: str, agent_id: str | None = ..., layer: str | None = ..., tags: list[str] | None = ..., filters: dict[str, Any] | None = ..., limit: int = ..., offset: int = ..., order_by: str = ..., order_direction: str = ...) -> Coroutine[Any, Any, list[dict[str, Any]]]
rae-core/rae_core/adapters/memory/storage.py:174: note:      Subclass:
rae-core/rae_core/adapters/memory/storage.py:174: note:          def list_memories(self, tenant_id: str, agent_id: str | None = ..., layer: str | None = ..., tags: list[str] | None = ..., limit: int = ..., offset: int = ...) -> Coroutine[Any, Any, list[dict[str, Any]]]
apps/memory_api/utils/circuit_breaker.py:136: error: Exception type must be derived from BaseException (or be a tuple of exception classes)  [misc]
apps/memory_api/services/memory_consolidation.py:100: error: Need type annotation for "results" (hint: "results: list[<type>] = ...")  [var-annotated]
apps/memory_api/services/memory_consolidation.py:158: error: Need type annotation for "results" (hint: "results: list[<type>] = ...")  [var-annotated]
apps/memory_api/services/memory_consolidation.py:224: error: Need type annotation for "results" (hint: "results: list[<type>] = ...")  [var-annotated]
apps/memory_api/services/memory_consolidation.py:605: error: "object" has no attribute "append"  [attr-defined]
apps/memory_api/services/memory_consolidation.py:688: error: Need type annotation for "memories" (hint: "memories: list[<type>] = ...")  [var-annotated]
apps/memory_api/services/importance_scoring.py:319: error: Incompatible return value type (got "dict[str, object]", expected "dict[str, int]")  [return-value]
apps/memory_api/services/importance_scoring.py:532: error: Need type annotation for "undervalued" (hint: "undervalued: list[<type>] = ...")  [var-annotated]
apps/memory_api/services/importance_scoring.py:596: error: Need type annotation for "archived" (hint: "archived: list[<type>] = ...")  [var-annotated]
apps/memory_api/services/hybrid_cache.py:140: error: Returning Any from function declared to return "dict[str, Any] | None"  [no-any-return]
apps/memory_api/services/graph_algorithms.py:357: error: Argument "key" to "max" has incompatible type overloaded function; expected "Callable[[int], SupportsDunderLT[Any] | SupportsDunderGT[Any]]"  [arg-type]
apps/memory_api/services/graph_algorithms.py:447: error: Need type annotation for "paths" (hint: "paths: list[<type>] = ...")  [var-annotated]
apps/memory_api/services/graph_algorithms.py:495: error: Need type annotation for "related" (hint: "related: list[<type>] = ...")  [var-annotated]
apps/memory_api/services/graph_algorithms.py:666: error: Need type annotation for "bridges" (hint: "bridges: list[<type>] = ...")  [var-annotated]
apps/memory_api/services/graph_algorithms.py:686: error: Need type annotation for "articulation_points" (hint: "articulation_points: list[<type>] = ...")  [var-annotated]
apps/memory_api/services/cost_controller.py:174: error: Returning Any from function declared to return "float"  [no-any-return]
apps/memory_api/services/cost_controller.py:284: error: Returning Any from function declared to return "float"  [no-any-return]
apps/memory_api/services/cost_controller.py:359: error: Returning Any from function declared to return "bool"  [no-any-return]
apps/memory_api/services/analytics.py:605: error: Need type annotation for "alerts" (hint: "alerts: list[<type>] = ...")  [var-annotated]
apps/memory_api/repositories/trigger_repository.py:278: error: Returning Any from function declared to return "bool"  [no-any-return]
apps/memory_api/repositories/trigger_repository.py:389: error: Returning Any from function declared to return "UUID"  [no-any-return]
apps/memory_api/repositories/trigger_repository.py:644: error: Returning Any from function declared to return "bool"  [no-any-return]
apps/memory_api/repositories/reflection_repository.py:237: error: Argument 1 to "append" of "list" has incompatible type "list[str]"; expected "str"  [arg-type]
apps/memory_api/repositories/reflection_repository.py:242: error: Argument 1 to "append" of "list" has incompatible type "int"; expected "str"  [arg-type]
apps/memory_api/repositories/reflection_repository.py:247: error: Argument 1 to "append" of "list" has incompatible type "float"; expected "str"  [arg-type]
apps/memory_api/repositories/reflection_repository.py:257: error: Argument 1 to "append" of "list" has incompatible type "UUID"; expected "str"  [arg-type]
apps/memory_api/repositories/reflection_repository.py:262: error: Argument 1 to "append" of "list" has incompatible type "datetime"; expected "str"  [arg-type]
apps/memory_api/repositories/reflection_repository.py:267: error: Argument 1 to "append" of "list" has incompatible type "datetime"; expected "str"  [arg-type]
apps/memory_api/repositories/reflection_repository.py:280: error: List item 0 has incompatible type "list[float]"; expected "str"  [list-item]
apps/memory_api/repositories/reflection_repository.py:280: error: List item 1 has incompatible type "int"; expected "str"  [list-item]
apps/memory_api/repositories/reflection_repository.py:286: error: Argument 1 to "append" of "list" has incompatible type "int"; expected "str"  [arg-type]
apps/memory_api/repositories/reflection_repository.py:458: error: Argument 1 to "append" of "list" has incompatible type "list[str]"; expected "UUID"  [arg-type]
apps/memory_api/repositories/reflection_repository.py:463: error: Argument 1 to "append" of "list" has incompatible type "float"; expected "UUID"  [arg-type]
apps/memory_api/repositories/reflection_repository.py:510: error: Argument 1 to "append" of "list" has incompatible type "float"; expected "list[str]"  [arg-type]
apps/memory_api/repositories/reflection_repository.py:659: error: Returning Any from function declared to return "UUID"  [no-any-return]
apps/memory_api/repositories/metrics_repository.py:61: error: Returning Any from function declared to return "int"  [no-any-return]
apps/memory_api/repositories/metrics_repository.py:356: error: Returning Any from function declared to return "int"  [no-any-return]
apps/memory_api/repositories/graph_repository_enhanced.py:631: error: Returning Any from function declared to return "UUID"  [no-any-return]
apps/memory_api/repositories/graph_repository.py:423: error: Returning Any from function declared to return "bool"  [no-any-return]
apps/memory_api/repositories/graph_repository.py:469: error: Returning Any from function declared to return "bool"  [no-any-return]
apps/memory_api/repositories/graph_repository.py:534: error: Argument "node_id" to "create_node" of "GraphRepository" has incompatible type "Any | None"; expected "str"  [arg-type]
apps/memory_api/repositories/graph_repository.py:535: error: Argument "label" to "create_node" of "GraphRepository" has incompatible type "Any | None"; expected "str"  [arg-type]
apps/memory_api/repositories/graph_repository.py:545: error: Argument "node_id" to "create_node" of "GraphRepository" has incompatible type "Any | None"; expected "str"  [arg-type]
apps/memory_api/repositories/graph_repository.py:546: error: Argument "label" to "create_node" of "GraphRepository" has incompatible type "Any | None"; expected "str"  [arg-type]
apps/memory_api/repositories/graph_repository.py:554: error: Argument "node_id" to "get_node_internal_id" of "GraphRepository" has incompatible type "Any | None"; expected "str"  [arg-type]
apps/memory_api/repositories/graph_repository.py:558: error: Argument "node_id" to "get_node_internal_id" of "GraphRepository" has incompatible type "Any | None"; expected "str"  [arg-type]
apps/memory_api/repositories/graph_repository.py:570: error: Argument "relation" to "create_edge" of "GraphRepository" has incompatible type "Any | None"; expected "str"  [arg-type]
apps/memory_api/repositories/graph_repository.py:664: error: Returning Any from function declared to return "bool"  [no-any-return]
apps/memory_api/repositories/graph_repository.py:783: error: Returning Any from function declared to return "bool"  [no-any-return]
apps/memory_api/repositories/graph_repository.py:843: error: Returning Any from function declared to return "int"  [no-any-return]
apps/memory_api/plugins/base.py:517: error: Module has no attribute "util"  [attr-defined]
apps/memory_api/plugins/base.py:520: error: Module has no attribute "util"  [attr-defined]
apps/memory_api/observability/pii_scrubber.py:270: error: Function "builtins.any" is not valid as a type  [valid-type]
apps/memory_api/observability/pii_scrubber.py:270: note: Perhaps you meant "typing.Any" instead of "any"?
rae-core/rae_core/adapters/sqlite/storage.py:142: error: Signature of "store_memory" incompatible with supertype "rae_core.interfaces.storage.IMemoryStorage"  [override]
rae-core/rae_core/adapters/sqlite/storage.py:142: note:      Superclass:
rae-core/rae_core/adapters/sqlite/storage.py:142: note:          def store_memory(self, content: str, layer: str, tenant_id: str, agent_id: str, tags: list[str] | None = ..., metadata: dict[str, Any] | None = ..., embedding: list[float] | None = ..., importance: float | None = ..., expires_at: Any | None = ...) -> Coroutine[Any, Any, UUID]
rae-core/rae_core/adapters/sqlite/storage.py:142: note:      Subclass:
rae-core/rae_core/adapters/sqlite/storage.py:142: note:          def store_memory(self, content: str, layer: str, tenant_id: str, agent_id: str, tags: list[str] | None = ..., metadata: dict[str, Any] | None = ..., embedding: list[float] | None = ..., importance: float | None = ...) -> Coroutine[Any, Any, UUID]
rae-core/rae_core/adapters/sqlite/storage.py:277: error: Signature of "list_memories" incompatible with supertype "rae_core.interfaces.storage.IMemoryStorage"  [override]
rae-core/rae_core/adapters/sqlite/storage.py:277: note:      Superclass:
rae-core/rae_core/adapters/sqlite/storage.py:277: note:          def list_memories(self, tenant_id: str, agent_id: str | None = ..., layer: str | None = ..., tags: list[str] | None = ..., filters: dict[str, Any] | None = ..., limit: int = ..., offset: int = ..., order_by: str = ..., order_direction: str = ...) -> Coroutine[Any, Any, list[dict[str, Any]]]
rae-core/rae_core/adapters/sqlite/storage.py:277: note:      Subclass:
rae-core/rae_core/adapters/sqlite/storage.py:277: note:          def list_memories(self, tenant_id: str, agent_id: str | None = ..., layer: str | None = ..., tags: list[str] | None = ..., limit: int = ..., offset: int = ...) -> Coroutine[Any, Any, list[dict[str, Any]]]
rae-core/rae_core/adapters/sqlite/storage.py:312: error: List item 0 has incompatible type "int"; expected "str"  [list-item]
rae-core/rae_core/adapters/sqlite/storage.py:312: error: List item 1 has incompatible type "int"; expected "str"  [list-item]
rae-core/rae_core/adapters/redis.py:12: error: Incompatible types in assignment (expression has type "None", variable has type Module)  [assignment]
rae-core/rae_core/adapters/memory/vector.py:175: error: Returning Any from function declared to return "list[float] | None"  [no-any-return]
apps/memory_api/services/evaluation_service.py:58: error: Incompatible default for argument "k_values" (default has type "None", argument has type "list[int]")  [assignment]
apps/memory_api/services/evaluation_service.py:58: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/evaluation_service.py:58: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/evaluation_service.py:307: error: Incompatible types in assignment (expression has type "floating[Any] | float", variable has type "float")  [assignment]
apps/memory_api/services/evaluation_service.py:343: error: Incompatible types in assignment (expression has type "floating[Any] | float", variable has type "float")  [assignment]
apps/memory_api/services/temporal_graph.py:417: error: Need type annotation for "bucket_changes" (hint: "bucket_changes: list[<type>] = ...")  [var-annotated]
apps/memory_api/services/temporal_graph.py:445: error: Need type annotation for "change_type_counts" (hint: "change_type_counts: dict[<type>, <type>] = ...")  [var-annotated]
apps/memory_api/services/temporal_graph.py:550: error: Need type annotation for "entity_connection_count" (hint: "entity_connection_count: dict[<type>, <type>] = ...")  [var-annotated]
apps/memory_api/plugins/examples/analytics_tracker.py:122: error: Incompatible types in "await" (actual type "dict[str, Any]", expected type "Awaitable[Any]")  [misc]
apps/memory_api/observability/telemetry_profiles.py:184: error: Incompatible return value type (got "ProfileConfig | None", expected "ProfileConfig")  [return-value]
apps/memory_api/observability/telemetry_profiles.py:204: error: Item "None" of "ProfileConfig | None" has no attribute "sampling_rate"  [union-attr]
apps/memory_api/observability/telemetry_profiles.py:216: error: Item "None" of "ProfileConfig | None" has no attribute "excluded_attributes"  [union-attr]
apps/memory_api/observability/telemetry_profiles.py:232: error: Item "None" of "ProfileConfig | None" has no attribute "max_attribute_length"  [union-attr]
apps/memory_api/observability/telemetry_profiles.py:237: error: Item "None" of "ProfileConfig | None" has no attribute "enable_pii_scrubbing"  [union-attr]
apps/memory_api/observability/telemetry_profiles.py:253: error: Item "None" of "ProfileConfig | None" has no attribute "export_format"  [union-attr]
apps/memory_api/observability/telemetry_profiles.py:254: error: Item "None" of "ProfileConfig | None" has no attribute "sampling_rate"  [union-attr]
apps/memory_api/observability/telemetry_profiles.py:255: error: Item "None" of "ProfileConfig | None" has no attribute "retention_days"  [union-attr]
apps/memory_api/observability/telemetry_profiles.py:256: error: Item "None" of "ProfileConfig | None" has no attribute "enable_metrics"  [union-attr]
apps/memory_api/observability/telemetry_profiles.py:257: error: Item "None" of "ProfileConfig | None" has no attribute "enable_logging"  [union-attr]
apps/memory_api/services/dashboard_websocket.py:67: error: Incompatible default for argument "event_types" (default has type "None", argument has type "list[DashboardEventType]")  [assignment]
apps/memory_api/services/dashboard_websocket.py:67: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/dashboard_websocket.py:67: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/dashboard_websocket.py:282: error: Incompatible default for argument "event_types" (default has type "None", argument has type "list[DashboardEventType]")  [assignment]
apps/memory_api/services/dashboard_websocket.py:282: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/dashboard_websocket.py:282: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/dashboard_websocket.py:383: error: Incompatible default for argument "actions" (default has type "None", argument has type "list[str]")  [assignment]
apps/memory_api/services/dashboard_websocket.py:383: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/dashboard_websocket.py:383: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/dashboard_websocket.py:407: error: Incompatible default for argument "actions" (default has type "None", argument has type "list[str]")  [assignment]
apps/memory_api/services/dashboard_websocket.py:407: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/dashboard_websocket.py:407: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/routes/token_savings.py:40: error: Unexpected keyword argument "start_date" for "get_summary" of "TokenSavingsService"  [call-arg]
apps/memory_api/services/token_savings_service.py:69: note: "get_summary" of "TokenSavingsService" defined here
apps/memory_api/routes/token_savings.py:40: error: Unexpected keyword argument "end_date" for "get_summary" of "TokenSavingsService"  [call-arg]
apps/memory_api/routes/event_triggers.py:91: error: Argument "event_types" to "create_trigger" of "TriggerRepository" has incompatible type "list[EventType]"; expected "list[str]"  [arg-type]
apps/memory_api/routes/event_triggers.py:91: note: "list" is invariant -- see https://mypy.readthedocs.io/en/stable/common_issues.html#variance
apps/memory_api/routes/event_triggers.py:91: note: Consider using "Sequence" instead, which is covariant
apps/memory_api/routes/event_triggers.py:92: error: Argument "conditions" to "create_trigger" of "TriggerRepository" has incompatible type "ConditionGroup | None"; expected "list[dict[str, Any]]"  [arg-type]
apps/memory_api/routes/event_triggers.py:93: error: Argument "actions" to "create_trigger" of "TriggerRepository" has incompatible type "list[ActionConfig]"; expected "list[dict[str, Any]]"  [arg-type]
apps/memory_api/routes/event_triggers.py:158: error: Incompatible types in assignment (expression has type "dict[str, Any]", target has type "str")  [assignment]
apps/memory_api/routes/event_triggers.py:160: error: Incompatible types in assignment (expression has type "list[dict[str, Any]]", target has type "str")  [assignment]
apps/memory_api/routes/event_triggers.py:162: error: Incompatible types in assignment (expression has type "int", target has type "str")  [assignment]
apps/memory_api/routes/event_triggers.py:166: error: Incompatible types in assignment (expression has type "bool", target has type "str")  [assignment]
apps/memory_api/routes/event_triggers.py:250: error: Incompatible default for argument "status_filter" (default has type "None", argument has type "str")  [assignment]
apps/memory_api/routes/event_triggers.py:250: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/routes/event_triggers.py:250: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/routes/event_triggers.py:373: error: Argument 1 to "UUID" has incompatible type "UUID"; expected "str | None"  [arg-type]
apps/memory_api/routes/event_triggers.py:380: error: Argument 1 to "UUID" has incompatible type "UUID"; expected "str | None"  [arg-type]
apps/memory_api/routes/event_triggers.py:426: error: Unexpected keyword argument "parallel_execution" for "create_workflow" of "WorkflowRepository"  [call-arg]
apps/memory_api/repositories/trigger_repository.py:501: note: "create_workflow" of "WorkflowRepository" defined here
apps/memory_api/routes/event_triggers.py:430: error: Argument "steps" to "create_workflow" of "WorkflowRepository" has incompatible type "list[WorkflowStep]"; expected "list[dict[str, Any]]"  [arg-type]
apps/memory_api/routes/evaluation.py:291: error: "CreateABTestRequest" has no attribute "hypothesis"  [attr-defined]
apps/memory_api/routes/evaluation.py:293: error: "ABTestVariant" has no attribute "config"  [attr-defined]
apps/memory_api/routes/evaluation.py:295: error: "ABTestVariant" has no attribute "config"  [attr-defined]
apps/memory_api/routes/evaluation.py:297: error: "CreateABTestRequest" has no attribute "min_sample_size"  [attr-defined]
apps/memory_api/routes/evaluation.py:299: error: "CreateABTestRequest" has no attribute "primary_metric"  [attr-defined]
apps/memory_api/routes/evaluation.py:300: error: "CreateABTestRequest" has no attribute "secondary_metrics"  [attr-defined]
apps/memory_api/routes/evaluation.py:301: error: "CreateABTestRequest" has no attribute "created_by"  [attr-defined]
apps/memory_api/routes/evaluation.py:302: error: "CreateABTestRequest" has no attribute "tags"  [attr-defined]
apps/memory_api/routes/evaluation.py:303: error: "CreateABTestRequest" has no attribute "metadata"  [attr-defined]
apps/memory_api/models/rbac.py:209: error: Missing positional argument "pool" in call to "RBACService"  [call-arg]
apps/memory_api/services/rbac_service.py:73: error: Incompatible default for argument "project_ids" (default has type "None", argument has type "list[str]")  [assignment]
apps/memory_api/services/rbac_service.py:73: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/rbac_service.py:73: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/observability/opentelemetry_config.py:162: error: Incompatible types in assignment (expression has type "OTLPSpanExporter", variable has type "ConsoleSpanExporter")  [assignment]
apps/memory_api/observability/opentelemetry_config.py:162: note: Error code "assignment" not covered by "type: ignore" comment
apps/memory_api/routes/dashboard.py:154: error: Argument 4 to "handle_connection" of "DashboardWebSocketService" has incompatible type "list[DashboardEventType] | None"; expected "list[DashboardEventType]"  [arg-type]
apps/memory_api/routes/dashboard.py:1234: error: Need type annotation for "entries" (hint: "entries: list[<type>] = ...")  [var-annotated]
apps/memory_api/utils/cost_tracker.py:50: error: "BudgetService" has no attribute "increment_usage"  [attr-defined]
apps/memory_api/models/__init__.py:19: error: Argument 1 to "module_from_spec" has incompatible type "ModuleSpec | None"; expected "ModuleSpec"  [arg-type]
apps/memory_api/models/__init__.py:20: error: Item "None" of "ModuleSpec | None" has no attribute "loader"  [union-attr]
apps/memory_api/models/__init__.py:20: error: Item "None" of "Loader | Any | None" has no attribute "exec_module"  [union-attr]
apps/memory_api/observability/rae_tracing.py:91: error: "type[RAETracingContext]" has no attribute "_task_name"  [attr-defined]
apps/memory_api/observability/memory_tracing.py:297: error: Module has no attribute "iscoroutinefunction"  [attr-defined]
apps/memory_api/services/scoring.py:64: error: Returning Any from function declared to return "float"  [no-any-return]
apps/memory_api/config.py:40: error: Incompatible types in assignment (expression has type "str | None", variable has type "str")  [assignment]
apps/memory_api/config.py:182: error: Extra keys ("env_file", "env_file_encoding") for TypedDict "ConfigDict"  [typeddict-unknown-key]
apps/memory_api/config.py:182: error: Incompatible types in assignment (expression has type "ConfigDict", base class "BaseSettings" defined the type as "SettingsConfigDict")  [assignment]
sdk/python/rae_memory_sdk/client.py:28: error: Extra keys ("env_file", "env_file_encoding") for TypedDict "ConfigDict"  [typeddict-unknown-key]
sdk/python/rae_memory_sdk/client.py:28: error: Incompatible types in assignment (expression has type "ConfigDict", base class "BaseSettings" defined the type as "SettingsConfigDict")  [assignment]
sdk/python/rae_memory_sdk/client.py:73: error: Returning Any from function declared to return "dict[str, Any]"  [no-any-return]
sdk/python/rae_memory_sdk/client.py:93: error: Returning Any from function declared to return "dict[str, Any]"  [no-any-return]
sdk/python/rae_memory_sdk/client.py:230: error: Incompatible return value type (got "dict[str, Any]", expected "list[dict[str, Any]]")  [return-value]
sdk/python/rae_memory_sdk/client.py:249: error: Incompatible return value type (got "dict[str, Any]", expected "list[dict[str, Any]]")  [return-value]
sdk/python/rae_memory_sdk/client.py:382: error: Incompatible types in assignment (expression has type "list[str]", target has type "str")  [assignment]
sdk/python/rae_memory_sdk/client.py:384: error: Incompatible types in assignment (expression has type "dict[str, Any]", target has type "str")  [assignment]
sdk/python/rae_memory_sdk/client.py:455: error: Incompatible types in assignment (expression has type "dict[str, Any]", target has type "Sequence[Collection[str]]")  [assignment]
sdk/python/rae_memory_sdk/client.py:527: error: Incompatible return value type (got "dict[str, Any]", expected "list[dict[str, Any]]")  [return-value]
sdk/python/rae_memory_sdk/client.py:854: error: Incompatible types in assignment (expression has type "list[str]", target has type "str")  [assignment]
sdk/python/rae_memory_sdk/client.py:856: error: Incompatible types in assignment (expression has type "dict[str, Any]", target has type "str")  [assignment]
sdk/python/rae_memory_sdk/client.py:900: error: Incompatible types in assignment (expression has type "dict[str, Any]", target has type "Sequence[Collection[str]]")  [assignment]
sdk/python/rae_memory_sdk/client.py:945: error: Incompatible return value type (got "dict[str, Any]", expected "list[dict[str, Any]]")  [return-value]
apps/memory_api/clients/rae_client.py:465: error: Incompatible types in assignment (expression has type "None", variable has type "CircuitBreaker")  [assignment]
apps/memory_api/clients/rae_client.py:472: error: Incompatible types in assignment (expression has type "None", variable has type "ResponseCache")  [assignment]
apps/memory_api/clients/rae_client.py:562: error: Returning Any from function declared to return "dict[str, Any]"  [no-any-return]
apps/memory_api/clients/rae_client.py:598: error: Returning Any from function declared to return "dict[str, Any]"  [no-any-return]
apps/memory_api/clients/rae_client.py:638: error: Returning Any from function declared to return "dict[str, Any]"  [no-any-return]
apps/memory_api/clients/rae_client.py:694: error: Argument 1 to "classify_error" has incompatible type "Exception | None"; expected "Exception"  [arg-type]
apps/memory_api/clients/rae_client.py:759: error: Incompatible types in assignment (expression has type "dict[str, Any]", target has type "int")  [assignment]
apps/memory_api/clients/rae_client.py:764: error: Incompatible types in assignment (expression has type "float", target has type "int")  [assignment]
apps/memory_api/clients/rae_client.py:766: error: Incompatible types in assignment (expression has type "float", target has type "int")  [assignment]
apps/memory_api/clients/rae_client.py:767: error: Incompatible types in assignment (expression has type "float", target has type "int")  [assignment]
apps/memory_api/clients/rae_client.py:770: error: Incompatible types in assignment (expression has type "float", target has type "int")  [assignment]
apps/memory_api/clients/rae_client.py:788: error: Incompatible default for argument "method" (default has type "None", argument has type "str")  [assignment]
apps/memory_api/clients/rae_client.py:788: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/clients/rae_client.py:788: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/clients/rae_client.py:788: error: Incompatible default for argument "path" (default has type "None", argument has type "str")  [assignment]
apps/llm/providers/qwen_provider.py:130: error: Unsupported target for indexed assignment ("Collection[str]")  [index]
apps/llm/providers/qwen_provider.py:133: error: Unsupported target for indexed assignment ("Collection[str]")  [index]
apps/llm/providers/qwen_provider.py:136: error: Unsupported target for indexed assignment ("Collection[str]")  [index]
apps/llm/providers/qwen_provider.py:140: error: Unsupported target for indexed assignment ("Collection[str]")  [index]
apps/llm/providers/qwen_provider.py:144: error: "Collection[str]" has no attribute "update"  [attr-defined]
apps/llm/providers/qwen_provider.py:266: error: Unsupported target for indexed assignment ("Collection[str]")  [index]
apps/llm/providers/qwen_provider.py:269: error: Unsupported target for indexed assignment ("Collection[str]")  [index]
apps/llm/providers/qwen_provider.py:272: error: "Collection[str]" has no attribute "update"  [attr-defined]
apps/llm/providers/ollama_provider.py:98: error: Unsupported target for indexed assignment ("object")  [index]
apps/llm/providers/ollama_provider.py:104: error: Unsupported target for indexed assignment ("object")  [index]
apps/llm/providers/ollama_provider.py:171: error: Unsupported target for indexed assignment ("object")  [index]
apps/memory_api/services/ml_service_client.py:117: error: Incompatible default for argument "base_url" (default has type "None", argument has type "str")  [assignment]
apps/memory_api/services/ml_service_client.py:117: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/ml_service_client.py:117: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/ml_service_client.py:170: error: Unexpected keyword argument "logging_level" for "before_sleep_log"; did you mean "log_level"?  [call-arg]
.venv/lib/python3.12/site-packages/tenacity/before_sleep.py:31: note: "before_sleep_log" defined here
apps/memory_api/services/ml_service_client.py:188: error: Returning Any from function declared to return "Response"  [no-any-return]
apps/memory_api/services/ml_service_client.py:242: error: Returning Any from function declared to return "dict[str, Any]"  [no-any-return]
apps/memory_api/services/ml_service_client.py:288: error: Returning Any from function declared to return "list[dict[str, str]]"  [no-any-return]
apps/memory_api/services/ml_service_client.py:331: error: Returning Any from function declared to return "dict[str, Any]"  [no-any-return]
apps/memory_api/services/ml_service_client.py:377: error: Returning Any from function declared to return "list[dict[str, Any]]"  [no-any-return]
apps/memory_api/services/context_cache.py:76: error: Need type annotation for "semantic_blocks" (hint: "semantic_blocks: dict[<type>, <type>] = ...")  [var-annotated]
apps/memory_api/services/context_cache.py:83: error: Need type annotation for "reflective_blocks" (hint: "reflective_blocks: dict[<type>, <type>] = ...")  [var-annotated]
apps/memory_api/services/llm/ollama.py:17: error: Incompatible default for argument "api_url" (default has type "None", argument has type "str")  [assignment]
apps/memory_api/services/llm/ollama.py:17: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/llm/ollama.py:17: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/security/auth.py:156: error: Returning Any from function declared to return "dict[Any, Any]"  [no-any-return]
apps/memory_api/security/auth.py:175: error: Returning Any from function declared to return "str | None"  [no-any-return]
apps/memory_api/security/auth.py:188: error: Returning Any from function declared to return "str | None"  [no-any-return]
apps/memory_api/api/v1/health.py:125: error: Unsupported operand types for / ("None" and "int")  [operator]
apps/memory_api/api/v1/health.py:125: note: Left operand is of type "Any | None"
apps/memory_api/api/v1/health.py:251: error: Argument 1 to "determine_overall_status" has incompatible type "dict[str, ComponentHealth | BaseException]"; expected "dict[str, ComponentHealth]"  [arg-type]
apps/memory_api/api/v1/health.py:287: error: Dict entry 1 has incompatible type "str": "str | None"; expected "str": "str"  [dict-item]
apps/memory_api/api/v1/health.py:334: error: Need type annotation for "db_stats" (hint: "db_stats: dict[<type>, <type>] = ...")  [var-annotated]
apps/memory_api/api/v1/health.py:336: error: Need type annotation for "vector_stats" (hint: "vector_stats: dict[<type>, <type>] = ...")  [var-annotated]
apps/memory_api/api/v1/health.py:343: error: Unsupported operand types for / ("None" and "int")  [operator]
apps/memory_api/api/v1/health.py:343: note: Left operand is of type "Any | None"
apps/memory_api/clients/rae_api.py:191: error: Incompatible types in assignment (expression has type "list[str]", target has type "str | None")  [assignment]
apps/memory_api/clients/rae_api.py:601: error: Incompatible default for argument "method" (default has type "None", argument has type "str")  [assignment]
apps/memory_api/clients/rae_api.py:601: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/clients/rae_api.py:601: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/clients/rae_api.py:601: error: Incompatible default for argument "path" (default has type "None", argument has type "str")  [assignment]
rae-core/rae_core/engine.py:76: error: Argument "strategies" to "HybridSearchEngine" has incompatible type "dict[str, VectorSearchStrategy]"; expected "dict[str, SearchStrategy]"  [arg-type]
rae-core/rae_core/engine.py:76: note: "dict" is invariant -- see https://mypy.readthedocs.io/en/stable/common_issues.html#variance
rae-core/rae_core/engine.py:76: note: Consider using "Mapping" instead, which is covariant in the value type
rae-core/rae_core/engine.py:201: error: Unexpected keyword argument "top_k" for "search" of "HybridSearchEngine"  [call-arg]
rae-core/rae_core/search/engine.py:77: note: "search" of "HybridSearchEngine" defined here
rae-core/rae_core/engine.py:201: error: Unexpected keyword argument "similarity_threshold" for "search" of "HybridSearchEngine"  [call-arg]
rae-core/rae_core/engine.py:211: error: "HybridSearchEngine" has no attribute "rerank"  [attr-defined]
rae-core/rae_core/engine.py:216: error: Incompatible return value type (got "list[tuple[UUID, float]]", expected "list[dict[str, Any]]")  [return-value]
apps/llm/providers/openai_provider.py:30: error: Incompatible default for argument "api_base" (default has type "None", argument has type "str")  [assignment]
apps/llm/providers/openai_provider.py:30: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/llm/providers/openai_provider.py:30: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/llm/providers/openai_provider.py:125: error: No overload variant of "create" of "AsyncCompletions" matches argument type "dict[str, object]"  [call-overload]
apps/llm/providers/openai_provider.py:125: note: Possible overload variants:
apps/llm/providers/openai_provider.py:125: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream: Literal[False] | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion]
apps/llm/providers/openai_provider.py:125: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: Literal[True], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, AsyncStream[ChatCompletionChunk]]
apps/llm/providers/openai_provider.py:125: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: bool, audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion | AsyncStream[ChatCompletionChunk]]
apps/llm/providers/openai_provider.py:216: error: No overload variant of "create" of "AsyncCompletions" matches argument type "dict[str, object]"  [call-overload]
apps/llm/providers/openai_provider.py:216: note: Possible overload variants:
apps/llm/providers/openai_provider.py:216: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream: Literal[False] | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion]
apps/llm/providers/openai_provider.py:216: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: Literal[True], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, AsyncStream[ChatCompletionChunk]]
apps/llm/providers/openai_provider.py:216: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: bool, audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion | AsyncStream[ChatCompletionChunk]]
apps/llm/providers/grok_provider.py:144: error: No overload variant of "create" of "AsyncCompletions" matches argument type "dict[str, object]"  [call-overload]
apps/llm/providers/grok_provider.py:144: note: Possible overload variants:
apps/llm/providers/grok_provider.py:144: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream: Literal[False] | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion]
apps/llm/providers/grok_provider.py:144: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: Literal[True], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, AsyncStream[ChatCompletionChunk]]
apps/llm/providers/grok_provider.py:144: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: bool, audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion | AsyncStream[ChatCompletionChunk]]
apps/llm/providers/grok_provider.py:251: error: No overload variant of "create" of "AsyncCompletions" matches argument type "dict[str, object]"  [call-overload]
apps/llm/providers/grok_provider.py:251: note: Possible overload variants:
apps/llm/providers/grok_provider.py:251: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream: Literal[False] | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion]
apps/llm/providers/grok_provider.py:251: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: Literal[True], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, AsyncStream[ChatCompletionChunk]]
apps/llm/providers/grok_provider.py:251: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: bool, audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion | AsyncStream[ChatCompletionChunk]]
apps/llm/providers/deepseek_provider.py:128: error: No overload variant of "create" of "AsyncCompletions" matches argument type "dict[str, object]"  [call-overload]
apps/llm/providers/deepseek_provider.py:128: note: Possible overload variants:
apps/llm/providers/deepseek_provider.py:128: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream: Literal[False] | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion]
apps/llm/providers/deepseek_provider.py:128: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: Literal[True], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, AsyncStream[ChatCompletionChunk]]
apps/llm/providers/deepseek_provider.py:128: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: bool, audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion | AsyncStream[ChatCompletionChunk]]
apps/llm/providers/deepseek_provider.py:231: error: No overload variant of "create" of "AsyncCompletions" matches argument type "dict[str, object]"  [call-overload]
apps/llm/providers/deepseek_provider.py:231: note: Possible overload variants:
apps/llm/providers/deepseek_provider.py:231: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream: Literal[False] | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion]
apps/llm/providers/deepseek_provider.py:231: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: Literal[True], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, AsyncStream[ChatCompletionChunk]]
apps/llm/providers/deepseek_provider.py:231: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: bool, audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion | AsyncStream[ChatCompletionChunk]]
apps/memory_api/core/reward.py:285: error: Returning Any from function declared to return "float"  [no-any-return]
apps/memory_api/core/reward.py:345: error: Returning Any from function declared to return "float"  [no-any-return]
apps/memory_api/core/reward.py:378: error: Returning Any from function declared to return "float"  [no-any-return]
apps/llm/providers/anthropic_provider.py:118: error: No overload variant of "create" of "AsyncMessages" matches argument type "dict[str, object]"  [call-overload]
apps/llm/providers/anthropic_provider.py:118: note: Possible overload variants:
apps/llm/providers/anthropic_provider.py:118: note:     def create(self, *, max_tokens: int, messages: Iterable[MessageParam], model: Literal['claude-3-7-sonnet-latest', 'claude-3-7-sonnet-20250219', 'claude-3-5-haiku-latest', 'claude-3-5-haiku-20241022', 'claude-haiku-4-5', 'claude-haiku-4-5-20251001', 'claude-sonnet-4-20250514', 'claude-sonnet-4-0', 'claude-4-sonnet-20250514', 'claude-sonnet-4-5', 'claude-sonnet-4-5-20250929', 'claude-opus-4-0', 'claude-opus-4-20250514', 'claude-4-opus-20250514', 'claude-opus-4-1-20250805', 'claude-3-opus-latest', 'claude-3-opus-20240229', 'claude-3-haiku-20240307'] | str, metadata: MetadataParam | Omit = ..., service_tier: Literal['auto', 'standard_only'] | Omit = ..., stop_sequences: SequenceNotStr[str] | Omit = ..., stream: Literal[False] | Omit = ..., system: str | Iterable[TextBlockParam] | Omit = ..., temperature: float | Omit = ..., thinking: ThinkingConfigEnabledParam | ThinkingConfigDisabledParam | Omit = ..., tool_choice: ToolChoiceAutoParam | ToolChoiceAnyParam | ToolChoiceToolParam | ToolChoiceNoneParam | Omit = ..., tools: Iterable[ToolParam | ToolBash20250124Param | ToolTextEditor20250124Param | ToolTextEditor20250429Param | ToolTextEditor20250728Param | WebSearchTool20250305Param] | Omit = ..., top_k: int | Omit = ..., top_p: float | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, Message]
apps/llm/providers/anthropic_provider.py:118: note:     def create(self, *, max_tokens: int, messages: Iterable[MessageParam], model: Literal['claude-3-7-sonnet-latest', 'claude-3-7-sonnet-20250219', 'claude-3-5-haiku-latest', 'claude-3-5-haiku-20241022', 'claude-haiku-4-5', 'claude-haiku-4-5-20251001', 'claude-sonnet-4-20250514', 'claude-sonnet-4-0', 'claude-4-sonnet-20250514', 'claude-sonnet-4-5', 'claude-sonnet-4-5-20250929', 'claude-opus-4-0', 'claude-opus-4-20250514', 'claude-4-opus-20250514', 'claude-opus-4-1-20250805', 'claude-3-opus-latest', 'claude-3-opus-20240229', 'claude-3-haiku-20240307'] | str, stream: Literal[True], metadata: MetadataParam | Omit = ..., service_tier: Literal['auto', 'standard_only'] | Omit = ..., stop_sequences: SequenceNotStr[str] | Omit = ..., system: str | Iterable[TextBlockParam] | Omit = ..., temperature: float | Omit = ..., thinking: ThinkingConfigEnabledParam | ThinkingConfigDisabledParam | Omit = ..., tool_choice: ToolChoiceAutoParam | ToolChoiceAnyParam | ToolChoiceToolParam | ToolChoiceNoneParam | Omit = ..., tools: Iterable[ToolParam | ToolBash20250124Param | ToolTextEditor20250124Param | ToolTextEditor20250429Param | ToolTextEditor20250728Param | WebSearchTool20250305Param] | Omit = ..., top_k: int | Omit = ..., top_p: float | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, AsyncStream[RawMessageStartEvent | RawMessageDeltaEvent | RawMessageStopEvent | RawContentBlockStartEvent | RawContentBlockDeltaEvent | RawContentBlockStopEvent]]
apps/llm/providers/anthropic_provider.py:118: note:     def create(self, *, max_tokens: int, messages: Iterable[MessageParam], model: Literal['claude-3-7-sonnet-latest', 'claude-3-7-sonnet-20250219', 'claude-3-5-haiku-latest', 'claude-3-5-haiku-20241022', 'claude-haiku-4-5', 'claude-haiku-4-5-20251001', 'claude-sonnet-4-20250514', 'claude-sonnet-4-0', 'claude-4-sonnet-20250514', 'claude-sonnet-4-5', 'claude-sonnet-4-5-20250929', 'claude-opus-4-0', 'claude-opus-4-20250514', 'claude-4-opus-20250514', 'claude-opus-4-1-20250805', 'claude-3-opus-latest', 'claude-3-opus-20240229', 'claude-3-haiku-20240307'] | str, stream: bool, metadata: MetadataParam | Omit = ..., service_tier: Literal['auto', 'standard_only'] | Omit = ..., stop_sequences: SequenceNotStr[str] | Omit = ..., system: str | Iterable[TextBlockParam] | Omit = ..., temperature: float | Omit = ..., thinking: ThinkingConfigEnabledParam | ThinkingConfigDisabledParam | Omit = ..., tool_choice: ToolChoiceAutoParam | ToolChoiceAnyParam | ToolChoiceToolParam | ToolChoiceNoneParam | Omit = ..., tools: Iterable[ToolParam | ToolBash20250124Param | ToolTextEditor20250124Param | ToolTextEditor20250429Param | ToolTextEditor20250728Param | WebSearchTool20250305Param] | Omit = ..., top_k: int | Omit = ..., top_p: float | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, Message | AsyncStream[RawMessageStartEvent | RawMessageDeltaEvent | RawMessageStopEvent | RawContentBlockStartEvent | RawContentBlockDeltaEvent | RawContentBlockStopEvent]]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "int"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "Iterable[MessageParam]"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "Literal['claude-3-7-sonnet-latest', 'claude-3-7-sonnet-20250219', 'claude-3-5-haiku-latest', 'claude-3-5-haiku-20241022', 'claude-haiku-4-5', 'claude-haiku-4-5-20251001', 'claude-sonnet-4-20250514', 'claude-sonnet-4-0', 'claude-4-sonnet-20250514', 'claude-sonnet-4-5', 'claude-sonnet-4-5-20250929', 'claude-opus-4-0', 'claude-opus-4-20250514', 'claude-4-opus-20250514', 'claude-opus-4-1-20250805', 'claude-3-opus-latest', 'claude-3-opus-20240229', 'claude-3-haiku-20240307'] | str"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "MetadataParam | Omit"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "str | Omit | None"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "Literal['auto', 'standard_only'] | Omit"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "SequenceNotStr[str] | Omit"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "str | Iterable[TextBlockParam] | Omit"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "float | Omit"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "int | Omit"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "ThinkingConfigEnabledParam | ThinkingConfigDisabledParam | Omit"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "ToolChoiceAutoParam | ToolChoiceAnyParam | ToolChoiceToolParam | ToolChoiceNoneParam | Omit"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "Iterable[ToolParam | ToolBash20250124Param | ToolTextEditor20250124Param | ToolTextEditor20250429Param | ToolTextEditor20250728Param | WebSearchTool20250305Param] | Omit"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "Mapping[str, str | Omit] | None"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "Mapping[str, object] | None"  [arg-type]
apps/llm/providers/anthropic_provider.py:225: error: Argument 1 to "stream" of "AsyncMessages" has incompatible type "**dict[str, object]"; expected "float | Timeout | NotGiven | None"  [arg-type]
apps/memory_api/core/metrics.py:155: error: Need type annotation for "reward_window"  [var-annotated]
apps/memory_api/core/metrics.py:156: error: Need type annotation for "quality_window"  [var-annotated]
apps/memory_api/core/metrics.py:323: error: Incompatible types in assignment (expression has type "floating[Any]", target has type "float")  [assignment]
apps/memory_api/core/metrics.py:377: error: Argument "key" to "sorted" has incompatible type "Callable[[dict[str, object]], object]"; expected "Callable[[dict[str, object]], SupportsDunderLT[Any] | SupportsDunderGT[Any]]"  [arg-type]
apps/memory_api/core/metrics.py:377: error: Incompatible return value type (got "object", expected "SupportsDunderLT[Any] | SupportsDunderGT[Any]")  [return-value]
apps/memory_api/core/metrics.py:401: error: Argument "key" to "sorted" has incompatible type "Callable[[dict[str, object]], object]"; expected "Callable[[dict[str, object]], SupportsDunderLT[Any] | SupportsDunderGT[Any]]"  [arg-type]
apps/memory_api/core/metrics.py:401: error: Incompatible return value type (got "object", expected "SupportsDunderLT[Any] | SupportsDunderGT[Any]")  [return-value]
apps/ml_service/services/triple_extraction.py:97: error: "None" not callable  [misc]
apps/ml_service/services/triple_extraction.py:181: error: Incompatible return value type (got "None", expected "str")  [return-value]
apps/ml_service/services/triple_extraction.py:204: error: Incompatible return value type (got "None", expected "str")  [return-value]
apps/ml_service/services/triple_extraction.py:244: error: "None" not callable  [misc]
apps/ml_service/services/triple_extraction.py:314: error: Incompatible return value type (got "list[dict[str, float | str | Any]]", expected "list[dict[str, str]]")  [return-value]
apps/ml_service/services/nlp_service.py:110: error: "None" not callable  [misc]
apps/ml_service/services/nlp_service.py:129: error: Item "float" of "float | str | Any" has no attribute "lower"  [union-attr]
apps/ml_service/services/nlp_service.py:151: error: Item "float" of "float | str | Any" has no attribute "lower"  [union-attr]
apps/ml_service/services/nlp_service.py:190: error: "None" not callable  [misc]
apps/memory_api/adapters/qdrant_adapter.py:38: error: "AsyncQdrantClient" has no attribute "cluster_info"  [attr-defined]
apps/memory_api/adapters/qdrant_adapter.py:92: error: Item "None" of "VectorParams | dict[str, VectorParams] | None" has no attribute "size"  [union-attr]
apps/memory_api/adapters/qdrant_adapter.py:93: error: Item "dict[str, VectorParams]" of "VectorParams | dict[str, VectorParams] | None" has no attribute "distance"  [union-attr]
apps/memory_api/adapters/qdrant_adapter.py:93: error: Item "None" of "VectorParams | dict[str, VectorParams] | None" has no attribute "distance"  [union-attr]
rae-core/rae_core/adapters/qdrant.py:13: error: Cannot assign to a type  [misc]
rae-core/rae_core/adapters/qdrant.py:13: error: Incompatible types in assignment (expression has type "None", variable has type "type[QdrantClient]")  [assignment]
rae-core/rae_core/adapters/qdrant.py:255: error: Value of type "dict[str, Any] | None" is not indexable  [index]
rae-core/rae_core/adapters/qdrant.py:276: error: Item "None" of "dict[str, Any] | None" has no attribute "get"  [union-attr]
rae-core/rae_core/adapters/qdrant.py:277: error: Incompatible return value type (got "list[float] | list[list[float]] | dict[str, list[float] | SparseVector | list[list[float]]] | None", expected "list[float] | None")  [return-value]
rae-core/rae_core/adapters/qdrant.py:298: error: Item "None" of "dict[str, Any] | None" has no attribute "get"  [union-attr]
rae-core/rae_core/adapters/qdrant.py:493: error: Returning Any from function declared to return "float"  [no-any-return]
apps/memory_api/observability/traced_qdrant.py:275: error: Incompatible return value type (got "int | CountResult", expected "int")  [return-value]
apps/memory_api/adapters/vector.py:25: error: Unexpected keyword argument "dimension" for "QdrantVectorStore"  [call-arg]
rae-core/rae_core/adapters/qdrant.py:37: note: "QdrantVectorStore" defined here
apps/memory_api/adapters/vector.py:26: error: Argument "client" to "QdrantVectorStore" has incompatible type "AsyncQdrantClient"; expected "QdrantClient | None"  [arg-type]
apps/llm/providers/gemini_provider.py:116: error: Incompatible types in assignment (expression has type "list[str]", target has type "float")  [assignment]
apps/llm/providers/gemini_provider.py:120: error: Incompatible types in assignment (expression has type "str", target has type "float")  [assignment]
apps/llm/providers/gemini_provider.py:128: error: Incompatible types in assignment (expression has type "list[dict[Any, Any]]", target has type "str")  [assignment]
apps/llm/providers/gemini_provider.py:130: error: Argument 1 to "GenerativeModel" has incompatible type "**dict[str, str]"; expected "google.ai.generativelanguage_v1beta.types.generative_service.GenerationConfig | GenerationConfigDict | google.generativeai.types.generation_types.GenerationConfig | None"  [arg-type]
apps/llm/providers/gemini_provider.py:130: error: Argument 1 to "GenerativeModel" has incompatible type "**dict[str, str]"; expected "ToolConfigDict | ToolConfig | None"  [arg-type]
apps/llm/providers/gemini_provider.py:138: error: Argument "history" to "start_chat" of "GenerativeModel" has incompatible type "list[dict[Any, Any]]"; expected "Iterable[Content | ContentDict] | None"  [arg-type]
apps/llm/providers/gemini_provider.py:140: error: Argument "generation_config" to "send_message_async" of "ChatSession" has incompatible type "dict[str, float]"; expected "google.ai.generativelanguage_v1beta.types.generative_service.GenerationConfig | GenerationConfigDict | google.generativeai.types.generation_types.GenerationConfig"  [arg-type]
apps/llm/providers/gemini_provider.py:146: error: Argument "generation_config" to "generate_content_async" of "GenerativeModel" has incompatible type "dict[str, float]"; expected "google.ai.generativelanguage_v1beta.types.generative_service.GenerationConfig | GenerationConfigDict | google.generativeai.types.generation_types.GenerationConfig | None"  [arg-type]
apps/llm/providers/gemini_provider.py:236: error: Incompatible types in assignment (expression has type "str", target has type "float")  [assignment]
apps/llm/providers/gemini_provider.py:244: error: Incompatible types in assignment (expression has type "list[dict[Any, Any]]", target has type "str")  [assignment]
apps/llm/providers/gemini_provider.py:246: error: Argument 1 to "GenerativeModel" has incompatible type "**dict[str, str]"; expected "google.ai.generativelanguage_v1beta.types.generative_service.GenerationConfig | GenerationConfigDict | google.generativeai.types.generation_types.GenerationConfig | None"  [arg-type]
apps/llm/providers/gemini_provider.py:246: error: Argument 1 to "GenerativeModel" has incompatible type "**dict[str, str]"; expected "ToolConfigDict | ToolConfig | None"  [arg-type]
apps/llm/providers/gemini_provider.py:251: error: Argument "history" to "start_chat" of "GenerativeModel" has incompatible type "list[dict[Any, Any]]"; expected "Iterable[Content | ContentDict] | None"  [arg-type]
apps/llm/providers/gemini_provider.py:253: error: Argument "generation_config" to "send_message_async" of "ChatSession" has incompatible type "dict[str, float]"; expected "google.ai.generativelanguage_v1beta.types.generative_service.GenerationConfig | GenerationConfigDict | google.generativeai.types.generation_types.GenerationConfig"  [arg-type]
apps/llm/providers/gemini_provider.py:258: error: Argument "generation_config" to "generate_content_async" of "GenerativeModel" has incompatible type "dict[str, float]"; expected "google.ai.generativelanguage_v1beta.types.generative_service.GenerationConfig | GenerationConfigDict | google.generativeai.types.generation_types.GenerationConfig | None"  [arg-type]
apps/memory_api/services/pii_scrubber.py:85: error: Argument "analyzer_results" to "anonymize" of "AnonymizerEngine" has incompatible type "list[presidio_analyzer.recognizer_result.RecognizerResult]"; expected "list[presidio_anonymizer.entities.engine.recognizer_result.RecognizerResult]"  [arg-type]
apps/memory_api/services/llm/openai.py:17: error: Incompatible default for argument "api_key" (default has type "None", argument has type "str")  [assignment]
apps/memory_api/services/llm/openai.py:17: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/llm/openai.py:17: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/llm/openai.py:40: error: Item "None" of "CompletionUsage | None" has no attribute "prompt_tokens"  [union-attr]
apps/memory_api/services/llm/openai.py:41: error: Item "None" of "CompletionUsage | None" has no attribute "completion_tokens"  [union-attr]
apps/memory_api/services/llm/openai.py:42: error: Item "None" of "CompletionUsage | None" has no attribute "total_tokens"  [union-attr]
apps/memory_api/services/llm/openai.py:46: error: Argument "text" to "LLMResult" has incompatible type "str | None"; expected "str"  [arg-type]
apps/memory_api/services/llm/openai.py:65: error: No overload variant of "create" of "AsyncCompletions" matches argument types "str", "list[dict[str, str]]", "type[BaseModel]"  [call-overload]
apps/memory_api/services/llm/openai.py:65: note: Possible overload variants:
apps/memory_api/services/llm/openai.py:65: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream: Literal[False] | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion]
apps/memory_api/services/llm/openai.py:65: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: Literal[True], audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, AsyncStream[ChatCompletionChunk]]
apps/memory_api/services/llm/openai.py:65: note:     def create(self, *, messages: Iterable[ChatCompletionDeveloperMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam], model: str | Literal['gpt-5.1', 'gpt-5.1-2025-11-13', 'gpt-5.1-codex', 'gpt-5.1-mini', 'gpt-5.1-chat-latest', 'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-2025-08-07', 'gpt-5-mini-2025-08-07', 'gpt-5-nano-2025-08-07', 'gpt-5-chat-latest', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'gpt-4.1-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano-2025-04-14', 'o4-mini', 'o4-mini-2025-04-16', 'o3', 'o3-2025-04-16', 'o3-mini', 'o3-mini-2025-01-31', 'o1', 'o1-2024-12-17', 'o1-preview', 'o1-preview-2024-09-12', 'o1-mini', 'o1-mini-2024-09-12', 'gpt-4o', 'gpt-4o-2024-11-20', 'gpt-4o-2024-08-06', 'gpt-4o-2024-05-13', 'gpt-4o-audio-preview', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-audio-preview-2025-06-03', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o-search-preview', 'gpt-4o-mini-search-preview', 'gpt-4o-search-preview-2025-03-11', 'gpt-4o-mini-search-preview-2025-03-11', 'chatgpt-4o-latest', 'codex-mini-latest', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'gpt-4-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-16k-0613'], stream: bool, audio: ChatCompletionAudioParam | Omit | None = ..., frequency_penalty: float | Omit | None = ..., function_call: Literal['none', 'auto'] | ChatCompletionFunctionCallOptionParam | Omit = ..., functions: Iterable[Function] | Omit = ..., logit_bias: dict[str, int] | Omit | None = ..., logprobs: bool | Omit | None = ..., max_completion_tokens: int | Omit | None = ..., max_tokens: int | Omit | None = ..., metadata: dict[str, str] | Omit | None = ..., modalities: list[Literal['text', 'audio']] | Omit | None = ..., n: int | Omit | None = ..., parallel_tool_calls: bool | Omit = ..., prediction: ChatCompletionPredictionContentParam | Omit | None = ..., presence_penalty: float | Omit | None = ..., prompt_cache_key: str | Omit = ..., prompt_cache_retention: Literal['in-memory', '24h'] | Omit | None = ..., reasoning_effort: Literal['none', 'minimal', 'low', 'medium', 'high'] | None | Omit | None = ..., response_format: ResponseFormatText | ResponseFormatJSONSchema | ResponseFormatJSONObject | Omit = ..., safety_identifier: str | Omit = ..., seed: int | Omit | None = ..., service_tier: Literal['auto', 'default', 'flex', 'scale', 'priority'] | Omit | None = ..., stop: str | SequenceNotStr[str] | Omit | None = ..., store: bool | Omit | None = ..., stream_options: ChatCompletionStreamOptionsParam | Omit | None = ..., temperature: float | Omit | None = ..., tool_choice: Literal['none', 'auto', 'required'] | ChatCompletionAllowedToolChoiceParam | ChatCompletionNamedToolChoiceParam | ChatCompletionNamedToolChoiceCustomParam | Omit = ..., tools: Iterable[ChatCompletionFunctionToolParam | ChatCompletionCustomToolParam] | Omit = ..., top_logprobs: int | Omit | None = ..., top_p: float | Omit | None = ..., user: str | Omit = ..., verbosity: Literal['low', 'medium', 'high'] | Omit | None = ..., web_search_options: WebSearchOptions | Omit = ..., extra_headers: Mapping[str, str | Omit] | None = ..., extra_query: Mapping[str, object] | None = ..., extra_body: object | None = ..., timeout: float | Timeout | NotGiven | None = ...) -> Coroutine[Any, Any, ChatCompletion | AsyncStream[ChatCompletionChunk]]
apps/memory_api/services/llm/openai.py:73: error: Returning Any from function declared to return "BaseModel"  [no-any-return]
apps/memory_api/services/llm/gemini.py:17: error: Incompatible default for argument "api_key" (default has type "None", argument has type "str")  [assignment]
apps/memory_api/services/llm/gemini.py:17: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/llm/gemini.py:17: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/llm/gemini.py:32: error: Need type annotation for "generative_model"  [var-annotated]
apps/memory_api/services/llm/gemini.py:64: error: Need type annotation for "generative_model"  [var-annotated]
apps/memory_api/services/llm/gemini.py:71: error: Returning Any from function declared to return "BaseModel"  [no-any-return]
apps/memory_api/services/llm/anthropic.py:17: error: Incompatible default for argument "api_key" (default has type "None", argument has type "str")  [assignment]
apps/memory_api/services/llm/anthropic.py:17: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/llm/anthropic.py:17: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/llm/anthropic.py:31: error: Missing positional argument "response_model" in call to "create" of "AsyncInstructor"  [call-arg]
apps/llm/broker/llm_router.py:66: error: Incompatible types in assignment (expression has type "Path", variable has type "str | None")  [assignment]
apps/llm/broker/llm_router.py:70: error: Argument 1 to "open" has incompatible type "str | None"; expected "int | str | bytes | PathLike[str] | PathLike[bytes]"  [arg-type]
apps/llm/broker/llm_router.py:71: error: Returning Any from function declared to return "dict[Any, Any]"  [no-any-return]
apps/llm/broker/llm_router.py:241: error: "Coroutine[Any, Any, AsyncIterator[LLMChunk]]" has no attribute "__aiter__" (not async iterable)  [attr-defined]
apps/llm/broker/llm_router.py:241: note: Maybe you forgot to use "await"?
apps/llm/tests/broker/test_llm_router.py:57: error: Unexpected keyword argument "prompt" for "LLMRequest"  [call-arg]
apps/llm/tests/broker/test_llm_router.py:62: error: Missing positional argument "raw" in call to "LLMResponse"  [call-arg]
apps/llm/broker/orchestrator.py:67: error: Incompatible types in assignment (expression has type "Path", variable has type "str | None")  [assignment]
apps/llm/broker/orchestrator.py:71: error: Argument 1 to "open" has incompatible type "str | None"; expected "int | str | bytes | PathLike[str] | PathLike[bytes]"  [arg-type]
apps/llm/broker/orchestrator.py:72: error: Returning Any from function declared to return "dict[Any, Any]"  [no-any-return]
apps/memory_api/services/llm/litellm_provider.py:72: error: Returning Any from function declared to return "BaseModel"  [no-any-return]
apps/reranker_service/main.py:55: error: Argument "key" to "sorted" has incompatible type "Callable[[RerankItem], float | None]"; expected "Callable[[RerankItem], SupportsDunderLT[Any] | SupportsDunderGT[Any]]"  [arg-type]
apps/reranker_service/main.py:55: error: Incompatible return value type (got "float | None", expected "SupportsDunderLT[Any] | SupportsDunderGT[Any]")  [return-value]
apps/ml_service/services/entity_resolution.py:57: error: Incompatible default for argument "similarity_threshold" (default has type "None", argument has type "float")  [assignment]
apps/ml_service/services/entity_resolution.py:57: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/ml_service/services/entity_resolution.py:57: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/ml_service/services/entity_resolution.py:105: error: Need type annotation for "clusters" (hint: "clusters: dict[<type>, <type>] = ...")  [var-annotated]
apps/ml_service/services/entity_resolution.py:186: error: Returning Any from function declared to return "ndarray[tuple[Any, ...], dtype[Any]]"  [no-any-return]
apps/ml_service/services/embedding_service.py:60: error: Item "None" of "SentenceTransformer | None" has no attribute "encode"  [union-attr]
apps/ml_service/services/embedding_service.py:80: error: Item "None" of "SentenceTransformer | None" has no attribute "get_sentence_embedding_dimension"  [union-attr]
apps/ml_service/services/embedding_service.py:80: error: Incompatible return value type (got "int | Any | None", expected "int")  [return-value]
apps/memory_api/services/vector_store/qdrant_store.py:131: error: Need type annotation for "index_values" (hint: "index_values: dict[<type>, <type>] = ...")  [var-annotated]
apps/memory_api/services/llm/orchestrator_adapter.py:96: error: Unexpected keyword argument "response_text" for "error" of "Logger"  [call-arg]
.venv/lib/python3.12/site-packages/mypy/typeshed/stdlib/logging/__init__.pyi:168: note: "error" of "Logger" defined here
apps/reranker_service/tests/test_main.py:33: error: Name "app" already defined (possibly by an import)  [no-redef]
apps/reranker_service/tests/test_main.py:36: error: Name "app" already defined (possibly by an import)  [no-redef]
apps/ml_service/tests/test_entity_resolution_service.py:46: error: "Callable[[], Any]" has no attribute "_EMBEDDING_MODEL"  [attr-defined]
apps/ml_service/tests/test_entity_resolution_service.py:52: error: "Callable[[], Any]" has no attribute "_EMBEDDING_MODEL"  [attr-defined]
apps/memory_api/services/rae_core_service.py:77: error: Argument 1 to "QdrantVectorStore" has incompatible type "AsyncQdrantClient"; expected "str"  [arg-type]
apps/memory_api/services/rae_core_service.py:313: error: Missing named argument "strategy" for "SearchResponse"  [call-arg]
apps/memory_api/services/rae_core_service.py:313: error: Missing named argument "total_found" for "SearchResponse"  [call-arg]
apps/memory_api/services/rae_core_service.py:313: error: Missing named argument "execution_time_ms" for "SearchResponse"  [call-arg]
apps/memory_api/services/hybrid_search.py:176: error: Incompatible types in "await" (actual type "list[list[float]]", expected type "Awaitable[Any]")  [misc]
apps/memory_api/services/hybrid_search.py:308: error: Unsupported operand types for - ("int" and "str")  [operator]
apps/memory_api/services/hybrid_search.py:308: note: Right operand is of type "int | str"
apps/memory_api/services/reflection_engine_v2.py:364: error: Unsupported left operand type for + ("None")  [operator]
apps/memory_api/services/reflection_engine_v2.py:364: note: Left operand is of type "list[str] | None"
apps/memory_api/services/query_analyzer.py:134: error: Incompatible default for argument "context" (default has type "None", argument has type "list[str]")  [assignment]
apps/memory_api/services/query_analyzer.py:134: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/query_analyzer.py:134: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/query_analyzer.py:135: error: Incompatible default for argument "user_preferences" (default has type "None", argument has type "dict[str, Any]")  [assignment]
apps/memory_api/services/query_analyzer.py:135: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/query_analyzer.py:135: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/query_analyzer.py:405: error: Incompatible types in assignment (expression has type "None", variable has type "int")  [assignment]
apps/memory_api/services/evaluator.py:93: error: Incompatible types in assignment (expression has type "None", variable has type "list[str]")  [assignment]
apps/memory_api/services/evaluator.py:297: error: Incompatible types in assignment (expression has type "BaseModel", variable has type "LLMEvaluationResponse")  [assignment]
apps/memory_api/services/entity_resolution.py:49: error: Incompatible default for argument "ml_client" (default has type "None", argument has type "MLServiceClient")  [assignment]
apps/memory_api/services/entity_resolution.py:49: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/entity_resolution.py:49: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/entity_resolution.py:50: error: Incompatible default for argument "graph_repository" (default has type "None", argument has type "GraphRepository")  [assignment]
apps/memory_api/services/entity_resolution.py:50: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/entity_resolution.py:50: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/entity_resolution.py:157: error: Incompatible return value type (got "BaseModel", expected "MergeDecision")  [return-value]
apps/memory_api/services/entity_resolution.py:170: error: Incompatible default for argument "canonical_name" (default has type "None", argument has type "str")  [assignment]
apps/memory_api/services/entity_resolution.py:170: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/entity_resolution.py:170: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/community_detection.py:47: error: Incompatible default for argument "graph_repository" (default has type "None", argument has type "GraphRepository")  [assignment]
apps/memory_api/services/community_detection.py:47: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/community_detection.py:47: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/community_detection.py:86: error: Need type annotation for "communities" (hint: "communities: dict[<type>, <type>] = ...")  [var-annotated]
apps/memory_api/services/community_detection.py:138: error: Argument 3 to "_store_super_node" of "CommunityDetectionService" has incompatible type "list[str]"; expected "list[int]"  [arg-type]
apps/memory_api/services/community_detection.py:162: error: Incompatible return value type (got "BaseModel", expected "CommunitySummary")  [return-value]
apps/memory_api/services/semantic_search.py:66: error: Incompatible default for argument "node_types" (default has type "None", argument has type "list[SemanticNodeType]")  [assignment]
apps/memory_api/services/semantic_search.py:66: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/semantic_search.py:66: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/semantic_search.py:67: error: Incompatible default for argument "domains" (default has type "None", argument has type "list[str]")  [assignment]
apps/memory_api/services/semantic_search.py:67: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/semantic_search.py:67: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/semantic_search.py:68: error: Incompatible default for argument "min_priority" (default has type "None", argument has type "int")  [assignment]
apps/memory_api/services/semantic_search.py:68: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/semantic_search.py:68: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/semantic_search.py:169: error: "MLServiceClient" has no attribute "get_embedding"; maybe "generate_embeddings"?  [attr-defined]
apps/memory_api/services/semantic_search.py:359: error: Incompatible default for argument "node_types" (default has type "None", argument has type "list[SemanticNodeType]")  [assignment]
apps/memory_api/services/semantic_search.py:359: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/semantic_search.py:359: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/semantic_search.py:360: error: Incompatible default for argument "domains" (default has type "None", argument has type "list[str]")  [assignment]
apps/memory_api/services/semantic_search.py:360: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/semantic_search.py:360: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/semantic_search.py:361: error: Incompatible default for argument "min_priority" (default has type "None", argument has type "int")  [assignment]
apps/memory_api/services/semantic_search.py:361: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/semantic_search.py:361: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/routes/reflections.py:154: error: "MLServiceClient" has no attribute "get_embedding"; maybe "generate_embeddings"?  [attr-defined]
apps/memory_api/routes/reflections.py:332: error: Need type annotation for "depth_distribution" (hint: "depth_distribution: dict[<type>, <type>] = ...")  [var-annotated]
apps/memory_api/workers/memory_maintenance.py:304: error: "BaseModel" has no attribute "summary"  [attr-defined]
apps/memory_api/workers/memory_maintenance.py:304: error: "BaseModel" has no attribute "key_topics"  [attr-defined]
apps/memory_api/workers/memory_maintenance.py:304: error: "BaseModel" has no attribute "sentiment"  [attr-defined]
apps/memory_api/services/context_builder.py:343: error: Argument 2 to "rank_memories_by_score" has incompatible type "list[MemoryScoreResultV3]"; expected "list[MemoryScoreResult]"  [arg-type]
apps/memory_api/services/context_builder.py:360: error: Incompatible types in assignment (expression has type "list[MemoryScoreResult]", variable has type "list[MemoryScoreResultV3]")  [assignment]
apps/memory_api/services/context_builder.py:365: error: Argument 2 to "rank_memories_by_score" has incompatible type "list[MemoryScoreResultV3]"; expected "list[MemoryScoreResult]"  [arg-type]
apps/memory_api/services/context_builder.py:370: error: Argument 1 to "sorted" has incompatible type "zip[tuple[dict[str, Any], float]]"; expected "Iterable[dict[str, Any]]"  [arg-type]
apps/memory_api/services/context_builder.py:371: error: Invalid index type "int" for "dict[str, Any]"; expected type "str"  [index]
apps/memory_api/services/context_builder.py:374: error: Invalid index type "int" for "dict[str, Any]"; expected type "str"  [index]
apps/memory_api/services/hybrid_search_service.py:170: error: Item "None" of "HybridSearchCache | None" has no attribute "get"  [union-attr]
apps/memory_api/services/hybrid_search_service.py:214: error: Argument "context" to "analyze_query" of "QueryAnalyzer" has incompatible type "list[str] | None"; expected "list[str]"  [arg-type]
apps/memory_api/services/hybrid_search_service.py:308: error: Incompatible types in assignment (expression has type "HybridSearchResult", variable has type "SearchResultItem")  [assignment]
apps/memory_api/services/hybrid_search_service.py:347: error: Item "None" of "HybridSearchCache | None" has no attribute "set"  [union-attr]
apps/memory_api/services/hybrid_search_service.py:355: error: Incompatible return value type (got "SearchResultItem", expected "HybridSearchResult")  [return-value]
apps/memory_api/services/hybrid_search_service.py:376: error: "MLServiceClient" has no attribute "get_embedding"; maybe "generate_embeddings"?  [attr-defined]
apps/memory_api/services/hybrid_search_service.py:670: error: Argument 1 to "append" of "list" has incompatible type "datetime"; expected "str"  [arg-type]
apps/memory_api/services/hybrid_search_service.py:675: error: Argument 1 to "append" of "list" has incompatible type "list[str]"; expected "str"  [arg-type]
apps/memory_api/services/hybrid_search_service.py:679: error: Argument 1 to "append" of "list" has incompatible type "int"; expected "str"  [arg-type]
apps/memory_api/dependencies.py:69: error: Returning Any from function declared to return "Redis[Any]"  [no-any-return]
apps/memory_api/dependencies.py:78: error: Returning Any from function declared to return "AsyncQdrantClient"  [no-any-return]
apps/memory_api/dependencies.py:121: error: Returning Any from function declared to return "RAECoreService"  [no-any-return]
apps/memory_api/services/reflection_engine.py:85: error: Incompatible default for argument "graph_repository" (default has type "None", argument has type "GraphRepository")  [assignment]
apps/memory_api/services/reflection_engine.py:85: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True
apps/memory_api/services/reflection_engine.py:85: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase
apps/memory_api/services/reflection_engine.py:120: error: "BaseModel" has no attribute "triples"  [attr-defined]
apps/memory_api/api/v2/memory.py:128: error: "SearchResult" has no attribute "id"  [attr-defined]
apps/memory_api/api/v2/memory.py:131: error: "SearchResult" has no attribute "layer"  [attr-defined]
apps/memory_api/api/v2/memory.py:132: error: "SearchResult" has no attribute "importance"  [attr-defined]
apps/memory_api/api/v2/memory.py:133: error: "SearchResult" has no attribute "tags"  [attr-defined]
apps/memory_api/api/v2/memory.py:141: error: "SearchResponse" has no attribute "synthesized_context"  [attr-defined]
apps/memory_api/api/v1/governance.py:438: error: Need type annotation for "alerts" (hint: "alerts: list[<type>] = ...")  [var-annotated]
apps/memory_api/api/v1/agent.py:67: error: Missing positional argument "rae_service" in call to "ContextBuilder"  [call-arg]
apps/memory_api/api/v1/graph.py:163: error: Missing positional argument "rae_service" in call to "ReflectionEngine"  [call-arg]
apps/memory_api/api/v1/graph.py:231: error: Missing positional argument "rae_service" in call to "ReflectionEngine"  [call-arg]
apps/memory_api/main.py:211: error: Argument 2 to "add_exception_handler" of "Starlette" has incompatible type "Callable[[Request, RateLimitExceeded], Coroutine[Any, Any, JSONResponse]]"; expected "Callable[[Request, Exception], Response | Awaitable[Response]] | Callable[[WebSocket, Exception], Awaitable[None]]"  [arg-type]
apps/memory_api/main.py:297: error: Cannot assign to a method  [method-assign]
Found 387 errors in 103 files (checked 279 source files)
