# Provider Configuration for Multi-Agent Orchestrator
# Configure LLM providers, routing preferences, and cost constraints

providers:
  # Claude (Anthropic) - Advanced reasoning, code generation
  # Using Claude because Gemini CLI has issues with orchestrator prompts
  claude:
    enabled: true
    default_model: claude-sonnet-4-5-20250929
    settings:
      # API key from environment variable
      api_key: ${ANTHROPIC_API_KEY}

  # Gemini (Google) - Uses default model from ~/.gemini/settings.json
  # Note: -m flag causes 404 errors, so we use whatever model is default
  gemini:
    enabled: true  # Enabled - uses default Gemini model
    default_model: gemini-default
    settings:
      cli_path: gemini
      rate_limit_delay: true
      min_delay: 1.0
      max_delay: 10.0

  # Ollama - Local models (free, but requires Ollama running)
  ollama:
    enabled: false  # Disabled by default (requires local setup)
    default_model: llama3:70b
    settings:
      # Ollama API endpoint
      endpoint: http://localhost:11434

# Routing Configuration
routing:
  # Prefer local models when available (cost-saving)
  prefer_local: false

  # Maximum cost per task (USD)
  max_cost_per_task: 1.0

  # Fallback provider if preferred is unavailable
  fallback_provider: claude

# Model Selection Strategy
# The orchestrator will:
# 1. Choose models based on task risk, complexity, and area
# 2. Prefer cheaper models when quality is sufficient
# 3. Use cross-review (different models review each other's work)
# 4. Respect cost constraints while maintaining quality

# Cost Optimization Tips:
# - Use gemini-2.0-flash for low-risk tasks (docs, simple code)
# - Use gemini-2.0-pro for medium-risk tasks (tests, APIs)
# - Use claude-sonnet-4-5 for high-risk tasks (core logic, algorithms)
# - Enable Ollama for free local inference (requires setup)
