# Architecture

This document provides a detailed overview of the RAE (Reflective Agentic Memory Engine) architecture, its core concepts, and data flows.

## High-Level Concepts

RAE's architecture is built around a few core concepts that enable sophisticated memory management for AI agents.

### Memory Layers

RAE organizes memories into three distinct layers:

-   **Episodic Memory**: This layer stores a chronological log of events, observations, and interactions. It's the agent's "short-term memory" and is crucial for understanding the context of recent events.
-   **Semantic Memory**: This layer stores facts, knowledge, and rules. It's the agent's "long-term memory" and is used to store information that is not tied to a specific time or event.
-   **Reflective Memory**: This layer stores higher-level insights and reflections generated by the agent. It's a meta-layer that helps the agent learn from its past experiences and improve its future actions.

### Memory Types

Memories are further categorized by type, which can include:

-   `event`: An interaction or observation.
-   `rule`: A guiding principle for the agent's behavior.
-   `reflection`: A higher-level insight generated by the agent.
-   And other custom types as needed.

### Tenancy

RAE is a multi-tenant system, which means it can securely store and manage data for multiple different agents or users. Each request to the API must include a `X-Tenant-ID` header, which is used to isolate data at the database level.

## Core Data Flows

There are two main data flows in RAE: storing and querying memories.

### Store Memory

The process of storing a memory is as follows:

1.  A client sends a `POST` request to the `/v1/memory/store` endpoint.
2.  The request is authenticated and validated by the FastAPI application.
3.  The content of the memory is sent to an embedding model to generate a vector embedding.
4.  The memory, along with its metadata and vector embedding, is stored in the PostgreSQL database.
5.  The vector embedding is also stored in the selected vector store (Qdrant or pgvector) for similarity searching.

```
Client → POST /v1/memory/store → MemoryService
  ├─ Pydantic model validation
  ├─ EmbeddingService.generate()
  ├─ Postgres.save(memory)
  └─ VectorStore.add(vector)
```

### Query Memory

The process of querying for memories is more complex and involves several steps:

1.  A client sends a `POST` request to the `/v1/memory/query` endpoint.
2.  The query is vectorized using the same embedding model.
3.  The vector store is searched for the most similar memories.
4.  (Optional) The retrieved memories are re-ranked by the `reranker-service` to improve relevance.
5.  The memories are passed through a PII (Personally Identifiable Information) scrubber.
6.  (Optional) The memories are passed to an LLM to generate a reflection or a direct answer.
7.  The final set of memories and any generated reflections are returned to the client.

```
Client → POST /v1/memory/query → MemoryService
  ├─ VectorStore.search()  (episodic/semantic)
  ├─ Reranker.rerank()     (optional)
  ├─ PIIScrubber.scrub()
  ├─ LLMService.generate() (for reflection, optional)
  ├─ ReflectionEngine.reflect() (optional write-back)
  └─ Response (context + answer + reflection metadata)
```

## Services

RAE is composed of several services that work together to provide the full set of memory capabilities.

-   **LLMService**: An abstraction layer for interacting with different LLM backends (Gemini, OpenAI, Ollama, Anthropic). The backend is selected via the `RAE_LLM_BACKEND` environment variable.
-   **VectorStore**: An abstraction for vector storage and search. The implementation (Qdrant or pgvector) is chosen via the `VECTOR_STORE_BACKEND` environment variable.
-   **ContextCache**: A Redis-based cache that stores the context of recent interactions to reduce token usage. It uses compression to minimize storage.
-   **BudgetService & CostController**: These services work together to monitor and control the costs associated with LLM usage.
-   **ReflectionEngine**: This service is responsible for generating higher-level insights and reflections from the agent's memories.

## Tenancy & RLS (Row Level Security)

Multi-tenancy is a core feature of RAE. The `X-Tenant-ID` header is used to derive the `tenant_id` for each request. This `tenant_id` is then used to enforce data isolation at the database level using PostgreSQL's Row Level Security (RLS) feature.

The following tables are protected by RLS:

-   `memories`
-   `memory_tags`
-   `knowledge_graph`
-   And other tenant-specific tables.

Here is an example of the RLS policy from `infra/postgres/ddl/002_rls.sql`:

```sql
CREATE POLICY tenant_isolation_policy ON memories
  FOR ALL
  USING (tenant_id = current_setting('app.current_tenant'));
```

## Observability

RAE exposes a variety of metrics via a `/metrics` endpoint, which is compatible with Prometheus. These metrics provide insights into the performance and usage of the system.

The project includes a pre-configured Grafana dashboard for visualizing these metrics. For more details on setting up and using the observability stack, please see the (forthcoming) `docs/observability.md` document.