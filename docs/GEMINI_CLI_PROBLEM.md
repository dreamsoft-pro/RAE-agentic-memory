# ğŸ”´ Problem z Gemini CLI - Analiza

## TL;DR

**Gemini CLI NIE DZIAÅA z orkiestratorem** z powodu bÅ‚Ä™du:
```
"Unable to submit request because thinking is not supported by this model"
```

**RozwiÄ…zanie:** UÅ¼yj Claude API zamiast Gemini CLI.

---

## ğŸ› BÅ‚Ä™dy KtÃ³re WidzieliÅ›my

### BÅ‚Ä…d #1: Parsing Error (pozorny)
```
[ERROR] [ImportProcessor] Could not find child token in parent raw content
```

**MyÅ›leliÅ›my Å¼e:** Problem z dÅ‚ugimi promptami/special chars
**PrÃ³bowaliÅ›my:**
- âœ… SkrÃ³cenie reguÅ‚ projektowych (73KB â†’ 5KB)
- âœ… Rozbicie duÅ¼ych zadaÅ„ na maÅ‚e
- âœ… WysyÅ‚anie promptu przez stdin zamiast `-p` flag

**Efekt:** Dalej ten sam bÅ‚Ä…d

### BÅ‚Ä…d #2: Prawdziwa Przyczyna
```json
{
  "error": {
    "code": 400,
    "message": "Unable to submit request because thinking is not supported by this model",
    "status": "INVALID_ARGUMENT"
  }
}
```

**Å¹rÃ³dÅ‚o:** `/tmp/gemini-client-error-Turn.run-sendMessageStream-*.json`

---

## ğŸ” Analiza Przyczyny

### Co siÄ™ dzieje:

1. **Gemini CLI dodaje wÅ‚asny kontekst:**
   - StrukturÄ™ katalogÃ³w projektu (~200 plikÃ³w)
   - Informacje o systemie operacyjnym
   - Working directory
   - Setup message: "This is the Gemini CLI. We are setting up the context..."

2. **Gemini CLI prÃ³buje uÅ¼yÄ‡ "thinking mode":**
   - Gemini CLI ma wbudowany advanced mode
   - Automatycznie wÅ‚Ä…cza extended thinking
   - Model `gemini-2.0-flash` **NIE WSPIERA** thinking mode

3. **Konflikt:**
   ```
   Gemini CLI context setup
   + Thinking mode request
   + Orchestrator prompt
   = ERROR 400: thinking not supported
   ```

---

## ğŸ’¡ Dlaczego Proste Testy DziaÅ‚aÅ‚y?

W `test_simple.py` prosty prompt dziaÅ‚aÅ‚:
```python
prompt = "What is 2+2? Answer in one word only."
```

**DziaÅ‚a bo:**
- KrÃ³tki prompt
- Brak kontekstu orkiestratora
- Gemini CLI nie wÅ‚Ä…cza thinking mode dla prostych promptÃ³w

**Nie dziaÅ‚a dla orkiestratora bo:**
- DÅ‚ugi prompt z project rules
- Strukturalny JSON output required
- System prompt + user prompt
- Gemini CLI recognition â†’ thinking mode â†’ ERROR

---

## ğŸš« Czego NIE moÅ¼na zrobiÄ‡

### âŒ Opcja 1: WyÅ‚Ä…czyÄ‡ thinking mode w Gemini CLI
**Problem:** Brak takiej opcji w CLI flags
```bash
gemini --help  # Nie ma --no-thinking ani podobnych
```

### âŒ Opcja 2: UÅ¼yÄ‡ innego modelu Gemini
**Problem:** Wszystkie modele majÄ… ten sam problem z CLI
- gemini-2.0-flash â†’ nie wspiera thinking
- gemini-2.0-pro â†’ nie wspiera thinking
- gemini-2.5-flash â†’ nie wspiera thinking
- gemini-3.0-pro-preview â†’ MOÅ»E wspieraÄ‡, ale preview/unstable

### âŒ Opcja 3: ObejÅ›Ä‡ CLI context
**Problem:** CLI zawsze dodaje swÃ³j setup context
- Nie da siÄ™ tego wyÅ‚Ä…czyÄ‡
- To wbudowane w CLI behavior

---

## âœ… Co MOÅ»NA zrobiÄ‡

### Opcja A: UÅ¼yj Claude API (ZALECANE) âœ…
```yaml
# .orchestrator/providers.yaml
providers:
  claude:
    enabled: true
    default_model: claude-sonnet-4-5-20250929

  gemini:
    enabled: false  # Disabled
```

**Zalety:**
- âœ… DziaÅ‚a niezawodnie
- âœ… Åšwietna jakoÅ›Ä‡ dla orkiestracji
- âœ… Brak problemÃ³w z promptami
- âœ… Support dla zÅ‚oÅ¼onych zadaÅ„

**Wady:**
- ğŸ’° Koszt: ~$0.003/$0.015 per 1K tokens
- ğŸ’° ~$0.01-0.10 per zadanie

---

### Opcja B: UÅ¼yj Gemini API (z API key)

Zamiast Gemini CLI, uÅ¼yj bezpoÅ›rednio Gemini API:

```python
# Wymaga google-generativeai package
import google.generativeai as genai

genai.configure(api_key="YOUR_GEMINI_API_KEY")
model = genai.GenerativeModel('gemini-2.0-flash')
response = model.generate_content(prompt)
```

**Zalety:**
- âœ… Brak CLI context overhead
- âœ… PeÅ‚na kontrola nad requestem
- âœ… Darmowe (generous free tier)

**Wady:**
- âš ï¸ Wymaga API key (nie browser auth)
- âš ï¸ Trzeba przepisaÄ‡ GeminiProvider
- âš ï¸ Nie testowane z orkiestratorem

---

### Opcja C: Ollama (local models)

```yaml
providers:
  ollama:
    enabled: true
    default_model: llama3:70b
    settings:
      endpoint: http://localhost:11434
```

**Zalety:**
- âœ… CaÅ‚kowicie darmowe
- âœ… Privacy - wszystko local
- âœ… Brak limitÃ³w API

**Wady:**
- âš ï¸ Wymaga lokalnej instalacji Ollama
- âš ï¸ Potrzebuje GPU/duÅ¼o RAM
- âš ï¸ Wolniejsze niÅ¼ API

---

## ğŸ“Š PorÃ³wnanie KosztÃ³w

| Provider | Model | Koszt/zadanie | NiezawodnoÅ›Ä‡ |
|----------|-------|---------------|--------------|
| Claude API | Sonnet 4.5 | ~$0.01-0.10 | âœ…âœ…âœ… Wysoka |
| Gemini CLI | 2.0-flash | $0 | âŒ Nie dziaÅ‚a |
| Gemini API | 2.0-flash | $0* | âœ… Powinna dziaÅ‚aÄ‡ |
| Ollama | Llama 3 70B | $0 | âœ…âœ… Åšrednia-Wysoka |

*Free tier: 15 RPM, 1M tokens/day

---

## ğŸ¯ Rekomendacja

**Dla produkcji:** Claude API
- Najlepsza niezawodnoÅ›Ä‡
- Sprawdzona jakoÅ›Ä‡
- Warto $0.01-0.10 per zadanie

**Dla local dev:** Ollama
- Darmowe
- Privacy
- Wymaga setup

**NIE uÅ¼ywaj:** Gemini CLI
- Nie dziaÅ‚a z orkiestratorem
- BÅ‚Ä™dy thinking mode
- Zmarnowany czas debugowania

---

## ğŸ“ Historia Debugowania

1. âœ… Znaleziono bÅ‚Ä…d parsing (prompt too long)
2. âœ… SkrÃ³cono prompty (73KB â†’ 5KB)
3. âœ… Zmieniono na stdin zamiast -p flag
4. âŒ Dalej ten sam bÅ‚Ä…d
5. âœ… Znaleziono error log: `/tmp/gemini-client-error-*.json`
6. âœ… Odkryto prawdziwÄ… przyczynÄ™: "thinking not supported"
7. âœ… PrzeÅ‚Ä…czono na Claude API
8. âœ… **DZIAÅA!**

---

## ğŸ”— Linki

- Error log: `/tmp/gemini-client-error-Turn.run-sendMessageStream-*.json`
- Gemini docs: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
- Claude API docs: https://docs.anthropic.com/claude/reference/

---

**Wniosek:** Gemini CLI nie nadaje siÄ™ do orkiestracji multi-agent. UÅ¼yj Claude API.
