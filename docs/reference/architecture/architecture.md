# Architecture

This document provides a detailed overview of the RAE (Reflective Agentic Memory Engine) architecture, its core concepts, and data flows.

## High-Level Concepts

RAE's architecture is built around a few core concepts that enable sophisticated memory management for AI agents.

### Memory Layers

RAE organizes memories into three distinct layers:

-   **Episodic Memory**: This layer stores a chronological log of events, observations, and interactions. It's the agent's "short-term memory" and is crucial for understanding the context of recent events.
-   **Semantic Memory**: This layer stores facts, knowledge, and rules. It's the agent's "long-term memory" and is used to store information that is not tied to a specific time or event.
-   **Reflective Memory**: This layer stores higher-level insights and reflections generated by the agent. It's a meta-layer that helps the agent learn from its past experiences and improve its future actions.

### Memory Types

Memories are further categorized by type, which can include:

-   `event`: An interaction or observation.
-   `rule`: A guiding principle for the agent's behavior.
-   `reflection`: A higher-level insight generated by the agent.
-   And other custom types as needed.

### Tenancy

RAE is a multi-tenant system, which means it can securely store and manage data for multiple different agents or users. Each request to the API must include a `X-Tenant-ID` header, which is used to isolate data at the database level.

## Core Data Flows

There are two main data flows in RAE: storing and querying memories.

### Store Memory

The process of storing a memory is as follows:

1.  A client sends a `POST` request to the `/v1/memory/store` endpoint.
2.  The request is authenticated and validated by the FastAPI application.
3.  The content of the memory is sent to an embedding model to generate a vector embedding.
4.  The memory, along with its metadata and vector embedding, is stored in the PostgreSQL database.
5.  The vector embedding is also stored in the selected vector store (Qdrant or pgvector) for similarity searching.

```
Client → POST /v1/memory/store → MemoryService
  ├─ Pydantic model validation
  ├─ EmbeddingService.generate()
  ├─ Postgres.save(memory)
  └─ VectorStore.add(vector)
```

### Query Memory

The process of querying for memories is more complex and involves several steps:

1.  A client sends a `POST` request to the `/v1/memory/query` endpoint.
2.  The query is vectorized using the same embedding model.
3.  The vector store is searched for the most similar memories.
4.  (Optional) The retrieved memories are re-ranked by the `reranker-service` to improve relevance.
5.  The memories are passed through a PII (Personally Identifiable Information) scrubber.
6.  (Optional) The memories are passed to an LLM to generate a reflection or a direct answer.
7.  The final set of memories and any generated reflections are returned to the client.

```
Client → POST /v1/memory/query → MemoryService
  ├─ VectorStore.search()  (episodic/semantic)
  ├─ Reranker.rerank()     (optional)
  ├─ PIIScrubber.scrub()
  ├─ LLMService.generate() (for reflection, optional)
  ├─ ReflectionEngine.reflect() (optional write-back)
  └─ Response (context + answer + reflection metadata)
```

## Services

RAE is composed of several services that work together to provide the full set of memory capabilities.

-   **LLMService**: An abstraction layer for interacting with different LLM backends (Gemini, OpenAI, Ollama, Anthropic). The backend is selected via the `RAE_LLM_BACKEND` environment variable.
-   **VectorStore**: An abstraction for vector storage and search. The implementation (Qdrant or pgvector) is chosen via the `RAE_VECTOR_BACKEND` environment variable.
-   **ContextCache**: A Redis-based cache that stores the context of recent interactions to reduce token usage. It uses compression to minimize storage.
-   **BudgetService & CostController**: These services work together to monitor and control the costs associated with LLM usage.
-   **ReflectionEngine**: This service is responsible for generating higher-level insights and reflections from the agent's memories.

## Tenancy & RLS (Row Level Security)

Multi-tenancy is a core feature of RAE. The `X-Tenant-ID` header is used to derive the `tenant_id` for each request. This `tenant_id` is then used to enforce data isolation at the database level using PostgreSQL's Row Level Security (RLS) feature.

The following tables are protected by RLS:

-   `memories`
-   `memory_tags`
-   `knowledge_graph`
-   And other tenant-specific tables.

Here is an example of the RLS policy from `infra/postgres/ddl/002_rls.sql`:

```sql
CREATE POLICY tenant_isolation_policy ON memories
  FOR ALL
  USING (tenant_id = current_setting('app.current_tenant'));
```

## Memory Lifecycle & Governance

RAE implements enterprise-grade memory lifecycle management through automated decay, importance scoring, and access tracking systems.

### Access Tracking

Every memory retrieval operation automatically updates two critical tracking fields:

- **`last_accessed_at`**: UTC timestamp of most recent access
- **`usage_count`**: Cumulative access counter

These fields are updated in batch operations for performance:

```
Client → /v1/memory/query or /v1/agent/execute
  ├─ VectorStore.search()
  ├─ Results returned to client
  └─ MemoryRepository.update_memory_access_stats(memory_ids[])
      └─ Batch UPDATE: usage_count++, last_accessed_at = NOW()
```

**Tracked Operations:**
- `/v1/memory/query` - Standard vector search
- `/v1/memory/query` with `use_graph=true` - Hybrid GraphRAG search
- `/v1/agent/execute` - Agent execution with memory retrieval

**Implementation:** The batch update uses PostgreSQL's `ANY` array operator to avoid N+1 queries:

```sql
UPDATE memories
SET usage_count = usage_count + 1,
    last_accessed_at = $1
WHERE id = ANY($2::uuid[])
  AND tenant_id = $3
```

### Importance Scoring

The `ImportanceScoringService` calculates dynamic importance scores using multiple weighted factors:

| Factor | Weight | Source |
|--------|--------|--------|
| Recency | 15% | `created_at` |
| Access Frequency | 20% | `usage_count` + `last_accessed_at` |
| Graph Centrality | 15% | Knowledge graph position |
| Semantic Relevance | 15% | Similarity to recent queries |
| User Rating | 10% | Explicit ratings |
| Consolidation | 10% | Reflective memory status |
| Manual Boost | 15% | Admin adjustments |

**Access Frequency Calculation:**
- Logarithmic scaling prevents high-volume memories from dominating
- Recency component: Decays over 30 days since last access
- Formula: `0.7 * log10(usage_count + 1) / log10(100) + 0.3 * (1 - days_since_access / 30)`

### Temporal Decay

Automated importance decay runs periodically (recommended: daily) to adjust scores based on temporal factors:

**Decay Strategies:**
1. **Protected Decay** (accessed within 7 days): `effective_rate = base_rate * 0.5`
2. **Normal Decay** (accessed 7-30 days ago): `effective_rate = base_rate`
3. **Accelerated Decay** (not accessed 30+ days): `effective_rate = base_rate * (1 + days_since_access / 30)`

**Example Decay Timeline** (base_rate = 0.01):

| Days Since Access | Daily Decay | 30-Day Impact |
|-------------------|-------------|---------------|
| 3 days | 0.5% | ~14% reduction |
| 15 days | 1.0% | ~26% reduction |
| 60 days | 3.0% | ~60% reduction |
| 90 days | 4.0% | ~70% reduction |

**Production Implementation:**

```python
# Celery beat task or cron job
from apps.memory_api.services.importance_scoring import ImportanceScoringService

scoring_service = ImportanceScoringService(db=pool)
await scoring_service.decay_importance(
    tenant_id=tenant_uuid,
    decay_rate=0.01,  # 1% per day
    consider_access_stats=True
)
```

### Memory Lifecycle States

```
┌─────────────┐
│   CREATED   │ importance = user-specified (default 0.5)
│             │ usage_count = 0
└──────┬──────┘
       │
       ▼
┌─────────────┐
│   ACTIVE    │ Accessed regularly
│             │ usage_count++
│             │ last_accessed_at updated
│             │ importance maintained/increased
└──────┬──────┘
       │
       ▼
┌─────────────┐
│   AGING     │ Not accessed recently (7-30 days)
│             │ importance decays at base rate
└──────┬──────┘
       │
       ▼
┌─────────────┐
│   STALE     │ Not accessed 30+ days
│             │ importance decays faster
│             │ candidate for archival
└──────┬──────┘
       │
       ▼
┌─────────────┐
│  ARCHIVED   │ importance < threshold (e.g., 0.1)
│             │ age > retention period (e.g., 90 days)
│             │ moved to archive or deleted
└─────────────┘
```

### Governance Integration

Memory access stats feed into the enterprise governance system:

- **Cost Tracking**: High `usage_count` memories identified for caching optimization
- **Budget Management**: Access patterns inform budget projections
- **Analytics**: Usage trends tracked per tenant/project
- **Compliance**: Audit trail of memory access for regulatory requirements

See `docs/guides/DEPLOYMENT.md` for decay configuration parameters.

## Observability

RAE exposes a variety of metrics via a `/metrics` endpoint, which is compatible with Prometheus. These metrics provide insights into the performance and usage of the system.

The project includes a pre-configured Grafana dashboard for visualizing these metrics. For more details on setting up and using the observability stack, please see the (forthcoming) `docs/observability.md` document.